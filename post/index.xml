<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on ğŸ‹ Lemon Zest</title>
    <link>https://leo-p.github.io/post/</link>
    <description>Recent content in Posts on ğŸ‹ Lemon Zest</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Fri, 01 Nov 2019 13:50:31 +0900</lastBuildDate>
    
	<atom:link href="https://leo-p.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>[Paper] Focal Loss</title>
      <link>https://leo-p.github.io/post/papers/2019-11-01-focal-loss/</link>
      <pubDate>Fri, 01 Nov 2019 13:50:31 +0900</pubDate>
      
      <guid>https://leo-p.github.io/post/papers/2019-11-01-focal-loss/</guid>
      <description>Focal Loss for Dense Object Detection ï¸ï¸â­ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸â­ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸â­ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸â­ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸â­ï¸ï¸ï¸ | arXiv, GitHub  The contribution of this paper is twofold.
First they introduce a new loss, called Focal Loss (FL) which reduces the usual Cross-Entropy (CE) loss for well-classified examples and puts more focus on hard, misclassified examples.
 Focal Loss for different parameters.   Then they move on to investigate why single-stage detector perform poorly than two-stage approach. They discover that it&amp;rsquo;s due to foreground-background class imbalance and use the Focal Loss to mitigate it.</description>
    </item>
    
    <item>
      <title>[Paper] Anchor Loss</title>
      <link>https://leo-p.github.io/post/papers/2019-11-01-anchor-loss/</link>
      <pubDate>Fri, 01 Nov 2019 08:56:02 +0900</pubDate>
      
      <guid>https://leo-p.github.io/post/papers/2019-11-01-anchor-loss/</guid>
      <description>Anchor Loss: Modulating Loss Scale Based on Prediction Difficulty â­ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸â­ï¸ï¸ï¸ï¸ï¸â­ï¸ | arXiv  This paper introduces a new loss based on ambiguity and prediction difficulty. Basically the network is penalized if a false prediction becomes significant. To do so they incorporate a measure of uncertainty and prediction difficulty into the loss.
This metric comes from from the confidence score gap between positive and negative labels.
 Anchor Loss Equation.   The Anchor Loss equation penalizes :</description>
    </item>
    
    <item>
      <title>[Paper] TensorMask</title>
      <link>https://leo-p.github.io/post/papers/2019-10-31-tensormask/</link>
      <pubDate>Thu, 31 Oct 2019 13:57:48 +0900</pubDate>
      
      <guid>https://leo-p.github.io/post/papers/2019-10-31-tensormask/</guid>
      <description>TensorMask: A Foundation for Dense Object Segmentation â­ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸â­ï¸ï¸ï¸ï¸ï¸â­ï¸ | arXiv, GitHub  This instance segmentation (differentiante between two objects) paper intend to bring to Mask R-CNN the same type of improvements that SSD/RetinaNet brought to Faster R-CNN: using dense sliding windows at fixed grid location.
For instance, Mask R-CNN first detect object bounding boxes, then crop and finally segment these regions. The goal here is to do everything at once.</description>
    </item>
    
    <item>
      <title>[Paper] End-to-End OCR</title>
      <link>https://leo-p.github.io/post/papers/2019-10-31-end-to-end-text-spotting/</link>
      <pubDate>Thu, 31 Oct 2019 13:41:01 +0900</pubDate>
      
      <guid>https://leo-p.github.io/post/papers/2019-10-31-end-to-end-text-spotting/</guid>
      <description>Towards Unconstrained End-to-End Text Spotting â­ï¸ï¸ï¸ï¸ï¸ï¸â­ï¸ï¸ï¸ï¸â­ï¸ï¸ï¸ï¸ï¸ï¸â­ï¸ï¸ï¸ï¸ï¸ï¸ | arXiv  This paper is motivated by a simple observation; most OCR engine proceed in two steps:
 Train a detector to identify the text boxe Crop on this box and use a traditional OCR engine  Their intent is thus to provide an end-to-end network to replace both components. This proves especially useful for text of irregular shape, curved text for instance.</description>
    </item>
    
    <item>
      <title>[Paper] ImageNet Pre-Training</title>
      <link>https://leo-p.github.io/post/papers/2019-10-31-rethinking-imagenet-pretraining/</link>
      <pubDate>Thu, 31 Oct 2019 11:41:15 +0900</pubDate>
      
      <guid>https://leo-p.github.io/post/papers/2019-10-31-rethinking-imagenet-pretraining/</guid>
      <description>Rethinking ImageNet Pre-Training â­ï¸ï¸ï¸ï¸ï¸ï¸â­ï¸ï¸ï¸ï¸â­ï¸ï¸ï¸ï¸ï¸ï¸â­ï¸ï¸ï¸ï¸ï¸ï¸ | arXiv  This paper asks the big questions, do we need to pre-train on ImageNet? To answer it they use the COCO dataset and compare results for modeles trained from ImageNet or from a random initialization.
The answers bellow have been modified, please refer to the full paper for the original quote.
Is ImageNet pre-training necessary?
Noâ€”with enough target data and computation. ImageNet help speed up convergence, but does not necessarily improve accuracy unless the target dataset is too small (Is ImageNet helpful?</description>
    </item>
    
    <item>
      <title>[Paper] MobileNetV3</title>
      <link>https://leo-p.github.io/post/papers/2019-10-31-searching-mobilenetv3/</link>
      <pubDate>Thu, 31 Oct 2019 10:08:42 +0900</pubDate>
      
      <guid>https://leo-p.github.io/post/papers/2019-10-31-searching-mobilenetv3/</guid>
      <description> Searching for MobileNetV3 â­ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸â­ï¸ï¸ï¸ï¸ï¸ | arXiv  The latest version in the MobileNets series optimized for small computing footprint. This time they combine:
 Network Architecture Search to find a coarse architecture NetAdapt to finetune each layer  Two networks are created and released:
 MobileNetV3-Large for high-resolution images MobileNetV3-Small for low-resolution images  Summary:
 Next generation MobileNet. Automatically optmize architecture and layers.  </description>
    </item>
    
    <item>
      <title>[Paper] Gaussian YOLOv3</title>
      <link>https://leo-p.github.io/post/papers/2019-10-31-yolov3-gaussian/</link>
      <pubDate>Thu, 31 Oct 2019 09:45:01 +0900</pubDate>
      
      <guid>https://leo-p.github.io/post/papers/2019-10-31-yolov3-gaussian/</guid>
      <description>ï¸ï¸ï¸Gaussian YOLOv3: An Accurate and Fast Object Detector Using Localization Uncertainty for Autonomous Driving â­ï¸ï¸ï¸ï¸ï¸â­ï¸â­ï¸ï¸ï¸ | arXiv, GitHub  This paper improves on the classic YOLOv3 model by focusing on the bouding boxes prodiction.
Namely, they model the boxes using a gaussian distribution which is able to better fit the object while at the same time output a reliability score which is then used in combination with the objectness and class score.</description>
    </item>
    
    <item>
      <title>[Paper] Single-Photon 3D Imaging</title>
      <link>https://leo-p.github.io/post/papers/2019-10-30-single-photon-3d/</link>
      <pubDate>Wed, 30 Oct 2019 12:22:07 +0900</pubDate>
      
      <guid>https://leo-p.github.io/post/papers/2019-10-30-single-photon-3d/</guid>
      <description>ï¸ï¸ï¸Asynchronous Single-Photon 3D Imaging â­ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸â­ï¸ï¸ï¸ï¸ï¸â­ï¸ | arXiv  This paper deals with the root of computer vision: the sensor. Specifically they elaborate on a new kind of sensor, the single-photon sensor.
The particularity of this sensor is that it provides only a yes or no signal, whether a photon is detected or not, contrary to usual sensors which provide a saturation range (0-255) for instance.
This leads to the development of new techniques which are particularly useful for low-light images.</description>
    </item>
    
    <item>
      <title>[Paper] Tracktor</title>
      <link>https://leo-p.github.io/post/papers/2019-10-30-tracking-without-bells-whistle/</link>
      <pubDate>Wed, 30 Oct 2019 12:05:31 +0900</pubDate>
      
      <guid>https://leo-p.github.io/post/papers/2019-10-30-tracking-without-bells-whistle/</guid>
      <description>ï¸ï¸ï¸Tracking Without Bells and Whistles â­ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸â­ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸â­ï¸ï¸ï¸ï¸ï¸â­ï¸â­ï¸ | arXiv, GitHub  The motivation of this paper is to directly use a standard detector, such as Faster R-CNN, and exploits its bounding box regression capabilities for tracking purpose. In effect they transform a detector into what&amp;rsquo;s they call a Tracktor. Notably, they don&amp;rsquo;t use tracking data.
They reach state of the art performance and exceed them especially on difficulte situations (occlusions, etc.</description>
    </item>
    
    <item>
      <title>[Paper] SinGAN</title>
      <link>https://leo-p.github.io/post/papers/2019-10-29-singan/</link>
      <pubDate>Tue, 29 Oct 2019 21:25:50 +0900</pubDate>
      
      <guid>https://leo-p.github.io/post/papers/2019-10-29-singan/</guid>
      <description>ï¸ï¸ï¸SinGAN, Learning a Generative Model from a Single Natural Image â­ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸â­ï¸ï¸ï¸ï¸ï¸â­ï¸â­ï¸ | arXiv, GitHub  SinGAN, short for Single GAN, is a GAN trained on a single image. It&amp;rsquo;s then used to generate sample from the same image.
 SinGAN sampling.   The same model can then be used to perform a lot of transformations including:
 image inpainting image editing image harmonization super-resolution video animation from one frame   SinGAN manipulations.</description>
    </item>
    
    <item>
      <title>[Paper] Mesh R-CNN</title>
      <link>https://leo-p.github.io/post/papers/2019-10-29-mesh-r-cnn/</link>
      <pubDate>Tue, 29 Oct 2019 17:42:41 +0900</pubDate>
      
      <guid>https://leo-p.github.io/post/papers/2019-10-29-mesh-r-cnn/</guid>
      <description>ï¸ï¸ï¸Mesh R-CNN â­ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸â­ï¸ï¸ï¸ï¸ï¸â­ï¸ | arXiv  This new paper generates consistent and high-fidelity 3D shapes from 2D images. This is a large improvement on previous methods that used iterative shape deformation.
They proceed as follow:
 Use a usual R-CNN to produce a bounding box and 2D mask. Implement a new branch to cubify the image. Generate a cubed mesh. Align mesh with image. Refine voxel to mesh several times.</description>
    </item>
    
    <item>
      <title>[Paper] Layer Wise Relevance Propagation</title>
      <link>https://leo-p.github.io/post/papers/2019-10-29-layer-wise-relevance-propagation/</link>
      <pubDate>Tue, 29 Oct 2019 17:13:48 +0900</pubDate>
      
      <guid>https://leo-p.github.io/post/papers/2019-10-29-layer-wise-relevance-propagation/</guid>
      <description>ï¸ï¸ï¸Towards best practice in explaining neural network decisions with LRP â­ï¸ï¸ï¸ï¸ï¸â­ï¸â­ï¸ | arXiv, GitHub  The LRP or Layer-wise Relevance Propagation aims to explain which parts of an image matter. They achieve this by using a backpropagation where neurons that contribute the most received an increased weight.
 Layer-wise Relevance Propagation   This technique is very useful for images but also for other network or methodologies.
 Meaningful parts of the image.</description>
    </item>
    
    <item>
      <title>[Paper] Extremal Perturbations</title>
      <link>https://leo-p.github.io/post/papers/2019-10-29-extremal-perturbations/</link>
      <pubDate>Tue, 29 Oct 2019 16:46:53 +0900</pubDate>
      
      <guid>https://leo-p.github.io/post/papers/2019-10-29-extremal-perturbations/</guid>
      <description>ï¸ï¸ï¸ï¸Understanding Deep Networks via Extremal Perturbations and Smooth Masks â­ï¸ï¸ï¸ï¸ï¸ï¸ï¸â­ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸â­ï¸ï¸ï¸ï¸ï¸â­ï¸â­ï¸ | arXiv, GitHub  Very cool paper diving into interpretability! They identify the most relevant parts of an image for a classifier. Or said differently, the parts that best explain the prediction score.
Instead of the traditional approach of a rolling occlusion mask they instead 1) select a zone, 2) blur the rest of the image and 3) compute the prediction score.</description>
    </item>
    
    <item>
      <title>[Blog] ICCV 2019</title>
      <link>https://leo-p.github.io/post/blogs/2019-10-27-best-of-iccv/</link>
      <pubDate>Sun, 27 Oct 2019 19:00:00 -0700</pubDate>
      
      <guid>https://leo-p.github.io/post/blogs/2019-10-27-best-of-iccv/</guid>
      <description>The best of the 2019 International Conference on Computer Vision.</description>
    </item>
    
    <item>
      <title>[Book] I Will Teach You to Be Rich - Ramit Sethi</title>
      <link>https://leo-p.github.io/post/books/2019-10-26-i-will-teach-you-to-be-rich/</link>
      <pubDate>Sat, 26 Oct 2019 09:30:00 +0200</pubDate>
      
      <guid>https://leo-p.github.io/post/books/2019-10-26-i-will-teach-you-to-be-rich/</guid>
      <description>â­ï¸â­ï¸ï¸ï¸ï¸â­ï¸â­ï¸ | I Will Teach You to Be Rich - Ramit Sethi   On the second edition of the book, Ramit goes over everything that makes up your personal finance. This book is cleary aimed for the american crowd, with a large chunk of it dediacted to credit card debt management and psychology. This is the section that least interested me.
On the other hand, what I particularly love is that this book explains the promise and pitfalls of a lot of US financial products: credit card, checking account, saving account, 401K, Roth IRA.</description>
    </item>
    
    <item>
      <title>[Blog] New Mac setup</title>
      <link>https://leo-p.github.io/post/blogs/2019-10-25-new-mac-setup/</link>
      <pubDate>Fri, 25 Oct 2019 09:30:00 -0700</pubDate>
      
      <guid>https://leo-p.github.io/post/blogs/2019-10-25-new-mac-setup/</guid>
      <description>I lost everything on my Mac, so here is a reinstallation from scratch.</description>
    </item>
    
  </channel>
</rss>