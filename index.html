<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>ğŸ‹ Lemon Zest</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="Leo Paillier" /><meta name="description" content="Lemon zest, personal website of Leo Paillier." /><meta name="keywords" content="leo paillier, paillier leo, blog, lemon zest" />






<meta name="generator" content="Hugo 0.59.0 with theme even" />


<link rel="canonical" href="https://leo-p.github.io/" />
  <link href="https://leo-p.github.io/index.xml" rel="alternate" type="application/rss+xml" title="ğŸ‹ Lemon Zest" />
  <link href="https://leo-p.github.io/index.xml" rel="feed" type="application/rss+xml" title="ğŸ‹ Lemon Zest" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">


<link href="/dist/even.c2a46f00.min.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="ğŸ‹ Lemon Zest" />
<meta property="og:description" content="Lemon zest, personal website of Leo Paillier." />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://leo-p.github.io/" />

<meta property="og:updated_time" content="2019-11-01T13:50:31+09:00" />
<meta itemprop="name" content="ğŸ‹ Lemon Zest">
<meta itemprop="description" content="Lemon zest, personal website of Leo Paillier.">

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="ğŸ‹ Lemon Zest"/>
<meta name="twitter:description" content="Lemon zest, personal website of Leo Paillier."/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">ğŸ‹ Lemon Zest</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">ğŸ‹ Lemon Zest</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <section id="posts" class="posts">
    <article class="post">
  <header class="post-header">
    <h1 class="post-title"><a class="post-link" href="/post/papers/2019-11-01-focal-loss/">[Paper] Focal Loss</a></h1>
    <div class="post-meta">
      <span class="post-time"> Fri, 01 Nov 2019 13:50:31 </span>
      <div class="post-category">
          <a href="/categories/paper-review/"> paper-review </a>
          </div>
        <span class="more-meta"> 125 words </span>
        <span class="more-meta"> 1 min read </span>
    </div>
  </header>
  
  <div class="post-content">
    <div class="post-summary">
      Focal Loss for Dense Object Detection ï¸ï¸â­ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸â­ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸â­ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸â­ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸â­ï¸ï¸ï¸ | arXiv, GitHub  The contribution of this paper is twofold.
First they introduce a new loss, called Focal Loss (FL) which reduces the usual Cross-Entropy (CE) loss for well-classified examples and puts more focus on hard, misclassified examples.
 Focal Loss for different parameters.   Then they move on to investigate why single-stage detector perform poorly than two-stage approach. They discover that it&rsquo;s due to foreground-background class imbalance and use the Focal Loss to mitigate it.
    </div>
    <div class="read-more">
      <a href="/post/papers/2019-11-01-focal-loss/" class="read-more-link">Read more...</a>
    </div>
  </div>
</article>

    <article class="post">
  <header class="post-header">
    <h1 class="post-title"><a class="post-link" href="/post/papers/2019-11-01-anchor-loss/">[Paper] Anchor Loss</a></h1>
    <div class="post-meta">
      <span class="post-time"> Fri, 01 Nov 2019 08:56:02 </span>
      <div class="post-category">
          <a href="/categories/paper-review/"> paper-review </a>
          </div>
        <span class="more-meta"> 126 words </span>
        <span class="more-meta"> 1 min read </span>
    </div>
  </header>
  
  <div class="post-content">
    <div class="post-summary">
      Anchor Loss: Modulating Loss Scale Based on Prediction Difficulty â­ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸â­ï¸ï¸ï¸ï¸ï¸â­ï¸ | arXiv  This paper introduces a new loss based on ambiguity and prediction difficulty. Basically the network is penalized if a false prediction becomes significant. To do so they incorporate a measure of uncertainty and prediction difficulty into the loss.
This metric comes from from the confidence score gap between positive and negative labels.
 Anchor Loss Equation.   The Anchor Loss equation penalizes :
    </div>
    <div class="read-more">
      <a href="/post/papers/2019-11-01-anchor-loss/" class="read-more-link">Read more...</a>
    </div>
  </div>
</article>

    <article class="post">
  <header class="post-header">
    <h1 class="post-title"><a class="post-link" href="/post/papers/2019-10-31-tensormask/">[Paper] TensorMask</a></h1>
    <div class="post-meta">
      <span class="post-time"> Thu, 31 Oct 2019 13:57:48 </span>
      <div class="post-category">
          <a href="/categories/paper-review/"> paper-review </a>
          </div>
        <span class="more-meta"> 145 words </span>
        <span class="more-meta"> 1 min read </span>
    </div>
  </header>
  
  <div class="post-content">
    <div class="post-summary">
      TensorMask: A Foundation for Dense Object Segmentation â­ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸â­ï¸ï¸ï¸ï¸ï¸â­ï¸ | arXiv, GitHub  This instance segmentation (differentiante between two objects) paper intend to bring to Mask R-CNN the same type of improvements that SSD/RetinaNet brought to Faster R-CNN: using dense sliding windows at fixed grid location.
For instance, Mask R-CNN first detect object bounding boxes, then crop and finally segment these regions. The goal here is to do everything at once.
    </div>
    <div class="read-more">
      <a href="/post/papers/2019-10-31-tensormask/" class="read-more-link">Read more...</a>
    </div>
  </div>
</article>

    <article class="post">
  <header class="post-header">
    <h1 class="post-title"><a class="post-link" href="/post/papers/2019-10-31-end-to-end-text-spotting/">[Paper] End-to-End OCR</a></h1>
    <div class="post-meta">
      <span class="post-time"> Thu, 31 Oct 2019 13:41:01 </span>
      <div class="post-category">
          <a href="/categories/paper-review/"> paper-review </a>
          </div>
        <span class="more-meta"> 115 words </span>
        <span class="more-meta"> 1 min read </span>
    </div>
  </header>
  
  <div class="post-content">
    <div class="post-summary">
      Towards Unconstrained End-to-End Text Spotting â­ï¸ï¸ï¸ï¸ï¸ï¸â­ï¸ï¸ï¸ï¸â­ï¸ï¸ï¸ï¸ï¸ï¸â­ï¸ï¸ï¸ï¸ï¸ï¸ | arXiv  This paper is motivated by a simple observation; most OCR engine proceed in two steps:
 Train a detector to identify the text boxe Crop on this box and use a traditional OCR engine  Their intent is thus to provide an end-to-end network to replace both components. This proves especially useful for text of irregular shape, curved text for instance.
    </div>
    <div class="read-more">
      <a href="/post/papers/2019-10-31-end-to-end-text-spotting/" class="read-more-link">Read more...</a>
    </div>
  </div>
</article>

    <article class="post">
  <header class="post-header">
    <h1 class="post-title"><a class="post-link" href="/post/papers/2019-10-31-rethinking-imagenet-pretraining/">[Paper] ImageNet Pre-Training</a></h1>
    <div class="post-meta">
      <span class="post-time"> Thu, 31 Oct 2019 11:41:15 </span>
      <div class="post-category">
          <a href="/categories/paper-review/"> paper-review </a>
          </div>
        <span class="more-meta"> 226 words </span>
        <span class="more-meta"> 2 mins read </span>
    </div>
  </header>
  
  <div class="post-content">
    <div class="post-summary">
      Rethinking ImageNet Pre-Training â­ï¸ï¸ï¸ï¸ï¸ï¸â­ï¸ï¸ï¸ï¸â­ï¸ï¸ï¸ï¸ï¸ï¸â­ï¸ï¸ï¸ï¸ï¸ï¸ | arXiv  This paper asks the big questions, do we need to pre-train on ImageNet? To answer it they use the COCO dataset and compare results for modeles trained from ImageNet or from a random initialization.
The answers bellow have been modified, please refer to the full paper for the original quote.
Is ImageNet pre-training necessary?
Noâ€”with enough target data and computation. ImageNet help speed up convergence, but does not necessarily improve accuracy unless the target dataset is too small (Is ImageNet helpful?
    </div>
    <div class="read-more">
      <a href="/post/papers/2019-10-31-rethinking-imagenet-pretraining/" class="read-more-link">Read more...</a>
    </div>
  </div>
</article>

    <article class="post">
  <header class="post-header">
    <h1 class="post-title"><a class="post-link" href="/post/papers/2019-10-31-searching-mobilenetv3/">[Paper] MobileNetV3</a></h1>
    <div class="post-meta">
      <span class="post-time"> Thu, 31 Oct 2019 10:08:42 </span>
      <div class="post-category">
          <a href="/categories/paper-review/"> paper-review </a>
          </div>
        <span class="more-meta"> 58 words </span>
        <span class="more-meta"> 1 min read </span>
    </div>
  </header>
  
  <div class="post-content">
    <div class="post-summary">
       Searching for MobileNetV3 â­ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸â­ï¸ï¸ï¸ï¸ï¸ | arXiv  The latest version in the MobileNets series optimized for small computing footprint. This time they combine:
 Network Architecture Search to find a coarse architecture NetAdapt to finetune each layer  Two networks are created and released:
 MobileNetV3-Large for high-resolution images MobileNetV3-Small for low-resolution images  Summary:
 Next generation MobileNet. Automatically optmize architecture and layers.  
    </div>
    <div class="read-more">
      <a href="/post/papers/2019-10-31-searching-mobilenetv3/" class="read-more-link">Read more...</a>
    </div>
  </div>
</article>

    <article class="post">
  <header class="post-header">
    <h1 class="post-title"><a class="post-link" href="/post/papers/2019-10-31-yolov3-gaussian/">[Paper] Gaussian YOLOv3</a></h1>
    <div class="post-meta">
      <span class="post-time"> Thu, 31 Oct 2019 09:45:01 </span>
      <div class="post-category">
          <a href="/categories/paper-review/"> paper-review </a>
          </div>
        <span class="more-meta"> 105 words </span>
        <span class="more-meta"> 1 min read </span>
    </div>
  </header>
  
  <div class="post-content">
    <div class="post-summary">
      ï¸ï¸ï¸Gaussian YOLOv3: An Accurate and Fast Object Detector Using Localization Uncertainty for Autonomous Driving â­ï¸ï¸ï¸ï¸ï¸â­ï¸â­ï¸ï¸ï¸ | arXiv, GitHub  This paper improves on the classic YOLOv3 model by focusing on the bouding boxes prodiction.
Namely, they model the boxes using a gaussian distribution which is able to better fit the object while at the same time output a reliability score which is then used in combination with the objectness and class score.
    </div>
    <div class="read-more">
      <a href="/post/papers/2019-10-31-yolov3-gaussian/" class="read-more-link">Read more...</a>
    </div>
  </div>
</article>

    <article class="post">
  <header class="post-header">
    <h1 class="post-title"><a class="post-link" href="/post/papers/2019-10-30-single-photon-3d/">[Paper] Single-Photon 3D Imaging</a></h1>
    <div class="post-meta">
      <span class="post-time"> Wed, 30 Oct 2019 12:22:07 </span>
      <div class="post-category">
          <a href="/categories/paper-review/"> paper-review </a>
          </div>
        <span class="more-meta"> 123 words </span>
        <span class="more-meta"> 1 min read </span>
    </div>
  </header>
  
  <div class="post-content">
    <div class="post-summary">
      ï¸ï¸ï¸Asynchronous Single-Photon 3D Imaging â­ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸â­ï¸ï¸ï¸ï¸ï¸â­ï¸ | arXiv  This paper deals with the root of computer vision: the sensor. Specifically they elaborate on a new kind of sensor, the single-photon sensor.
The particularity of this sensor is that it provides only a yes or no signal, whether a photon is detected or not, contrary to usual sensors which provide a saturation range (0-255) for instance.
This leads to the development of new techniques which are particularly useful for low-light images.
    </div>
    <div class="read-more">
      <a href="/post/papers/2019-10-30-single-photon-3d/" class="read-more-link">Read more...</a>
    </div>
  </div>
</article>

    <article class="post">
  <header class="post-header">
    <h1 class="post-title"><a class="post-link" href="/post/papers/2019-10-30-tracking-without-bells-whistle/">[Paper] Tracktor</a></h1>
    <div class="post-meta">
      <span class="post-time"> Wed, 30 Oct 2019 12:05:31 </span>
      <div class="post-category">
          <a href="/categories/paper-review/"> paper-review </a>
          </div>
        <span class="more-meta"> 133 words </span>
        <span class="more-meta"> 1 min read </span>
    </div>
  </header>
  
  <div class="post-content">
    <div class="post-summary">
      ï¸ï¸ï¸Tracking Without Bells and Whistles â­ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸â­ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸â­ï¸ï¸ï¸ï¸ï¸â­ï¸â­ï¸ | arXiv, GitHub  The motivation of this paper is to directly use a standard detector, such as Faster R-CNN, and exploits its bounding box regression capabilities for tracking purpose. In effect they transform a detector into what&rsquo;s they call a Tracktor. Notably, they don&rsquo;t use tracking data.
They reach state of the art performance and exceed them especially on difficulte situations (occlusions, etc.
    </div>
    <div class="read-more">
      <a href="/post/papers/2019-10-30-tracking-without-bells-whistle/" class="read-more-link">Read more...</a>
    </div>
  </div>
</article>

    <article class="post">
  <header class="post-header">
    <h1 class="post-title"><a class="post-link" href="/post/papers/2019-10-29-singan/">[Paper] SinGAN</a></h1>
    <div class="post-meta">
      <span class="post-time"> Tue, 29 Oct 2019 21:25:50 </span>
      <div class="post-category">
          <a href="/categories/paper-review/"> paper-review </a>
          </div>
        <span class="more-meta"> 123 words </span>
        <span class="more-meta"> 1 min read </span>
    </div>
  </header>
  
  <div class="post-content">
    <div class="post-summary">
      ï¸ï¸ï¸SinGAN, Learning a Generative Model from a Single Natural Image â­ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸â­ï¸ï¸ï¸ï¸ï¸â­ï¸â­ï¸ | arXiv, GitHub  SinGAN, short for Single GAN, is a GAN trained on a single image. It&rsquo;s then used to generate sample from the same image.
 SinGAN sampling.   The same model can then be used to perform a lot of transformations including:
 image inpainting image editing image harmonization super-resolution video animation from one frame   SinGAN manipulations.
    </div>
    <div class="read-more">
      <a href="/post/papers/2019-10-29-singan/" class="read-more-link">Read more...</a>
    </div>
  </div>
</article>

    </section>
  
  <nav class="pagination">
    
    <a class="next" href="/page/2/">
        <span class="next-text">Next</span>
        <i class="iconfont icon-right"></i>
      </a>
  </nav>
        </div>
        

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="mailto:leo.paillier&#43;lemon-zest@gmail.com" class="iconfont icon-email" title="email"></a>
      <a href="https://www.linkedin.com/in/leopaillier/" class="iconfont icon-linkedin" title="linkedin"></a>
      <a href="https://github.com/leo-p/" class="iconfont icon-github" title="github"></a>
  <a href="https://leo-p.github.io/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2019
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">Leo Paillier</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>
<script type="text/javascript" src="/dist/even.26188efa.min.js"></script>


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-151013843-1', 'auto');
	ga('set', 'anonymizeIp', true);
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>







</body>
</html>
