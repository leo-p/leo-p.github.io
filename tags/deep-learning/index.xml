<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>deep-learning on ğŸ‹ Lemon Zest</title>
    <link>https://leo-p.github.io/tags/deep-learning/</link>
    <description>Recent content in deep-learning on ğŸ‹ Lemon Zest</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 30 Oct 2019 12:05:31 +0900</lastBuildDate>
    
	<atom:link href="https://leo-p.github.io/tags/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Tracking Without Bells Whistle</title>
      <link>https://leo-p.github.io/post/papers/2019-10-30-tracking-without-bells-whistle/</link>
      <pubDate>Wed, 30 Oct 2019 12:05:31 +0900</pubDate>
      
      <guid>https://leo-p.github.io/post/papers/2019-10-30-tracking-without-bells-whistle/</guid>
      <description>ï¸ï¸ï¸Tracking Without Bells and Whistles â­ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸â­ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸â­ï¸ï¸ï¸ï¸ï¸â­ï¸â­ï¸ | arXiv, GitHub  The motivation of this paper is to directly use a standard detector, such as Faster R-CNN, and exploits its bounding box regression capabilities for tracking purpose. In effect they transform a detector into what&amp;rsquo;s they call a Tracktor. Notably, they don&amp;rsquo;t use tracking data.
They reach state of the art performance and exceed them especially on difficulte situations (occlusions, etc.</description>
    </item>
    
    <item>
      <title>SinGAN</title>
      <link>https://leo-p.github.io/post/papers/2019-10-29-singan/</link>
      <pubDate>Tue, 29 Oct 2019 21:25:50 +0900</pubDate>
      
      <guid>https://leo-p.github.io/post/papers/2019-10-29-singan/</guid>
      <description>ï¸ï¸ï¸SinGAN, Learning a Generative Model from a Single Natural Image â­ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸â­ï¸ï¸ï¸ï¸ï¸â­ï¸â­ï¸ | arXiv, GitHub  SinGAN, short for Single GAN, is a GAN trained on a single image. It&amp;rsquo;s then used to generate sample from the same image.
 SinGAN sampling.   The same model can then be used to perform a lot of transformations including:
 image inpainting image editing image harmonization super-resolution video animation from one frame   SinGAN manipulations.</description>
    </item>
    
    <item>
      <title>Mesh R-CNN</title>
      <link>https://leo-p.github.io/post/papers/2019-10-29-mesh-r-cnn/</link>
      <pubDate>Tue, 29 Oct 2019 17:42:41 +0900</pubDate>
      
      <guid>https://leo-p.github.io/post/papers/2019-10-29-mesh-r-cnn/</guid>
      <description>ï¸ï¸ï¸Mesh R-CNN â­ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸â­ï¸ï¸ï¸ï¸ï¸â­ï¸ | arXiv  This new paper generates consistent and high-fidelity 3D shapes from 2D images. This is a large improvement on previous methods that used iterative shape deformation.
They proceed as follow:
 Use a usual R-CNN to produce a bounding box and 2D mask. Implement a new branch to cubify the image. Generate a cubed mesh. Align mesh with image. Refine voxel to mesh several times.</description>
    </item>
    
    <item>
      <title>Layer Wise Relevance Propagation</title>
      <link>https://leo-p.github.io/post/papers/2019-10-29-layer-wise-relevance-propagation/</link>
      <pubDate>Tue, 29 Oct 2019 17:13:48 +0900</pubDate>
      
      <guid>https://leo-p.github.io/post/papers/2019-10-29-layer-wise-relevance-propagation/</guid>
      <description>ï¸ï¸ï¸Towards best practice in explaining neural network decisions with LRP â­ï¸ï¸ï¸ï¸ï¸â­ï¸â­ï¸ | arXiv, GitHub  The LRP or Layer-wise Relevance Propagation aims to explain which parts of an image matter. They achieve this by using a backpropagation where neurons that contribute the most received an increased weight.
 Layer-wise Relevance Propagation   This technique is very useful for images but also for other network or methodologies.
 Meaningful parts of the image.</description>
    </item>
    
    <item>
      <title>Extremal Perturbations</title>
      <link>https://leo-p.github.io/post/papers/2019-10-29-extremal-perturbations/</link>
      <pubDate>Tue, 29 Oct 2019 16:46:53 +0900</pubDate>
      
      <guid>https://leo-p.github.io/post/papers/2019-10-29-extremal-perturbations/</guid>
      <description>ï¸ï¸ï¸ï¸Understanding Deep Networks via Extremal Perturbations and Smooth Masks â­ï¸ï¸ï¸ï¸ï¸ï¸ï¸â­ï¸ï¸ï¸ï¸ï¸ï¸ï¸ï¸â­ï¸ï¸ï¸ï¸ï¸â­ï¸â­ï¸ | arXiv, GitHub  Very cool paper diving into interpretability! They identify the most relevant parts of an image for a classifier. Or said differently, the parts that best explain the prediction score.
Instead of the traditional approach of a rolling occlusion mask they instead 1) select a zone, 2) blur the rest of the image and 3) compute the prediction score.</description>
    </item>
    
    <item>
      <title>ICCV 2019</title>
      <link>https://leo-p.github.io/post/2019-10-27-best-of-iccv/</link>
      <pubDate>Sun, 27 Oct 2019 19:00:00 -0700</pubDate>
      
      <guid>https://leo-p.github.io/post/2019-10-27-best-of-iccv/</guid>
      <description>The best of the 2019 International Conference on Computer Vision.</description>
    </item>
    
  </channel>
</rss>